## One LEGO at a time: Explaining the Math of How Neural Networks Learn

>A **neural network** is a clever arrangement of linear and non-linear modules. When we choose and connect them wisely,
we have a powerful tool to approximate any mathematical function. For example one that **separates classes with a non-linear
decision boundary**.

A topic that is not always explained in depth, despite of its intuitive and modular nature, is the
**backpropagation technique** responsible for updating trainable parameters. Letâ€™s build a neural network from scratch
to see the internal functioning of a neural network using **LEGO pieces as a modular analogy**, one brick at a time. The
code can be found in this repository: https://github.com/omar-florez/scratch_mlp

---
[Omar U. Florez](https://www.linkedin.com/in/omar-u-florez-35338015/)
<br/>
[![CC0](https://i.creativecommons.org/p/zero/1.0/88x31.png)](https://creativecommons.org/publicdomain/zero/1.0/)
